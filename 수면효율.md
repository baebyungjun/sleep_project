# 리눅스

#### 서버접속

ssh -p 22 root@169.56.89.210 / tkHhD4UmcQsJ
ssh -p 22 humandeep@169.56.89.210 / humandeeP@916

#### gpu 사용량 확인

nvidia-smi -l 1 (1초마다 확인)

#### 경로확인

pwd

#### activate

conda info --envs  (확인하기)
conda activate p36  (라이브러리 환경)

#### PGCN 후

파일돌리고 *.pc, *.pkl 파일, save폴더 지우기

# Video 전처리





# TSN

Annotation, Video 전처리 후

dataset(ucf101, hmdb51, thumos, kinetics)에 따라 class 수 정의

TSN모델 정의	

​	_prepare_base_model (input에 따른 size, mean, std 정의)

​	prepare_tsn(마지막 layer 의 fc layer 정의)

​	consensusModule

model

​	crop_size, scale_size, input_mean, input_std 호출

​	get_optim_policies로 weight, bias 호출

​	get_augentation 실행

​    	GroupMultiScaleCrop  ->  GroupRandomHorizontalFlip 으로 crop 진행

Normalize 진행

train_loader

​	TSNDataSet 선언

​		path, list_file, num_segment, new_length, modality, image_tmpl(이미지경로), transform, random_shift, test_mode

loss function, optimizer 정의

​	CrossEntropyLoss(), SGD

train

==========training==========

partialBN = True로 지정

model의 train으로 switch

dataset.py 의 __ getitem__  실행

​	sample_indices

​		전체 영상의 frame을 segment로 나누어서 동일한 길이로 나눈다

​		이후 해당 구간에서 random한 위치에서 하나의 frame을 샘플링한다

​		구해진 segment 수만큼의 frame index를 return 한다

​	get 함수 실행

​		point를 통해서 해당 video에서 동일한 index의 image를 load 한다

​		segment 수 만큼의 이미지를 저장하고, 이를 transform 파이프라인에 넣어준다

​			transform 파이프라인

​				- GroupMultiScaleCrop	

​				- GroupRandomHorizontalFlip

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

input, target을 구하고 정의한 model 을 사용하여 training후 output, loss 구한다(model forward)

calculate loss, get accuracy

==========testing==========

해당 변수들 가져온다

tsn model 선언

cropping

​	GroupOverSample 사용

test dataloader

​	switch to test mode

dataset.py 의 __ getitem__  실행

​	test_indices

​		segment로 image sampling 수행

​	get 함수 실행

​		tranform 파이프라인

​				- GroupOversampling

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

test dataloader 사용해서 data와 label을 가져온다

eval_video

csv file로 저장





# BSN

#### TEM (Temporal Evaluation Module)

Input : TSN output(*.csv)





 

Ouput : [frame, action, start, end] (*.csv)



#### PGM (Proposals Generation Module)

Input : PEM output (*.csv)







Output : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



#### PEM (Proposals Evaluation Module)

Input : PGM output (*.csv)







Output : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



* BSN_post_processing => thumos14_bsn_results.csv 생성 

​											[f-end, f-init, score, video-frames, video-name, match-iou, match-ioa, match-xmin, match-xmax]

#### Proposal list

jupyter notebook file (D:\JupyterProject\Generate_Proposal_list.ipynb)

train_proposal_list : PGM   /   test_proposal_list : PEM 으로 생성

Output : train_proposal_list.txt , test_proposal_list.txt



# I3D







# PGCN

Input : BSN output data (proposal_list) + I3D output data (RGB, flow)