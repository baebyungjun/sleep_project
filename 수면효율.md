# 리눅스

#### 서버접속

```
ssh -p 22 root@169.56.89.210 / tkHhD4UmcQsJ
ssh -p 22 humandeep@169.56.89.210 / humandeeP@916
```



#### gpu 사용량 확인

```
nvidia-smi -l 1 (1초마다 확인)
```



#### 경로확인

```
pwd
```



#### activate

```
conda info --envs  (확인하기)
conda activate p36  (라이브러리 환경)
```



#### PGCN 후

파일돌리고 *.pc, *.pkl 파일, save폴더 지우기

# Video 전처리





# TSN

**Input** : Annotation, Video 전처리

dataset(ucf101, hmdb51, thumos, kinetics)에 따라 class 수 정의

TSN모델 정의	

​	_prepare_base_model (input에 따른 size, mean, std 정의)

​	prepare_tsn(마지막 layer 의 fc layer 정의)

​	consensusModule

model

​	crop_size, scale_size, input_mean, input_std 호출

​	get_optim_policies로 weight, bias 호출

​	get_augentation 실행

​    	GroupMultiScaleCrop  ->  GroupRandomHorizontalFlip 으로 crop 진행

Normalize 진행

train_loader

​	TSNDataSet 선언

​		path, list_file, num_segment, new_length, modality, image_tmpl(이미지경로), transform, random_shift, test_mode

loss function, optimizer 정의

​	CrossEntropyLoss(), SGD

train

==========training==========

partialBN = True로 지정

model의 train으로 switch

dataset.py 의 __ getitem__  실행

​	sample_indices

​		전체 영상의 frame을 segment로 나누어서 동일한 길이로 나눈다

​		이후 해당 구간에서 random한 위치에서 하나의 frame을 샘플링한다

​		구해진 segment 수만큼의 frame index를 return 한다

​	get 함수 실행

​		point를 통해서 해당 video에서 동일한 index의 image를 load 한다

​		segment 수 만큼의 이미지를 저장하고, 이를 transform 파이프라인에 넣어준다

​			transform 파이프라인

​				- GroupMultiScaleCrop	

​				- GroupRandomHorizontalFlip

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

input, target을 구하고 정의한 model 을 사용하여 training후 output, loss 구한다(model forward)

calculate loss, get accuracy

==========testing==========

해당 변수들 가져온다

tsn model 선언

cropping

​	GroupOverSample 사용

test dataloader

​	switch to test mode

dataset.py 의 __ getitem__  실행

​	test_indices

​		segment로 image sampling 수행

​	get 함수 실행

​		tranform 파이프라인

​				- GroupOversampling

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

test dataloader 사용해서 data와 label을 가져온다

eval_video

csv file로 저장

**Output** : *.csv





# BSN

#### TEM (Temporal Evaluation Module)

**Input** : TSN output(*.csv)





 

**Ouput** : [frame, action, start, end] (*.csv)



#### PGM (Proposals Generation Module)

**Input** : PEM output (*.csv)







**Output** : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



#### PEM (Proposals Evaluation Module)

**Input** : PGM output (*.csv)







**Output** : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



* BSN_post_processing => thumos14_bsn_results.csv 생성 

​											[f-end, f-init, score, video-frames, video-name, match-iou, match-ioa, match-xmin, match-xmax]

#### Proposal list

jupyter notebook file (D:\JupyterProject\Generate_Proposal_list.ipynb)

train_proposal_list : PGM   /   test_proposal_list : PEM 으로 생성

**Output** : train_proposal_list.txt , test_proposal_list.txt



# I3D







# PGCN

**Input** : BSN output data (proposal_list) + I3D output data (RGB, flow feature)

PCGN_train.py

​	model 정의

```
model = PGCN(model_configs, graph_configs)
```

​	train data 정의 (train_loader = torch.utils.data.DataLoader(PGCNDataSet()))

```
train_loader = torch.utils.data.DataLoader(
        PGCNDataSet(dataset_configs, graph_configs,
                    prop_file=dataset_configs['train_prop_file'],
                    prop_dict_path=dataset_configs['train_dict_path'],
                    ft_path=dataset_configs['train_ft_path'],
                    epoch_multiplier=dataset_configs['training_epoch_multiplier'],
                    test_mode=False),
        batch_size=args.batch_size, shuffle=True,
        num_workers=args.workers, pin_memory=True, drop_last=True)
```

​		proposal list의 proposal들을 나누고 분리해서 저장한다.

​		fore ground : incomplete : back ground = 1 : 6 : 1 로 나눈다

​		video 마다 아래 형태로 저장

​			- path

​			- frame

​			- gt_box [label, start, end]

​			- prop_box [label, iou, ioa, start, end]

​		-prepare_iou_dict() 실행 후 [act_iou_dict, act_dis_dict, prop_dict]를 pickle형태로 저장

```
pickle.dump([self.act_iou_dict, self.act_dis_dict, self.prop_dict], open(self.prop_dict_path, "wb"))
```

​			(iou_dict : 상관관계,     dis_dict : 거리)

​	loss function 선언

```
activity_criterion = torch.nn.CrossEntropyLoss().cuda()
completeness_criterion = CompletenessLoss().cuda()
regression_criterion = ClassWiseRegressionLoss().cuda()
```

​	optimizer와 weight, learning rate을 선언

```
adjust_learning_rate(optimizer, epoch, args.lr_steps)
```

train 진행

```
train(train_loader, model, activity_criterion, completeness_criterion, regression_criterion, optimizer, epoch)
```

​	get_training_data를 통해서 하나의 데이터를 가져온다.

​		video centric sampling 진행

​			video에서 fg, incomp, bg를 분리 -> 8번 sampling(1:6:1)

​			sampling할 type에서 랜덤으로 데이터(proposal)를 하나 추출한다

​			추출한 proposal을 루트노드라고 가정하고, 루트노드를 통해 자식노드를 sampling한다

​			추출한 루트노드와 비슷한 iou를 갖거나 dis를 가진 proposal 8개(iou), 2개(dis) sampling하고 그중 4개 랜덤 sampling 한다

​			이를 5번 반복 (4*5+1 = 21) => 총 168개 proposal사용 (fg : 21개,  incomp : 126개,  bf : 21개) 

​		load_prop_data -> **return** indices, label, reg_targets, type











**output** : pred_dump.pc file 



# 결과 도출

jupyter notebook -> PGCN_predict.ipynb

![image-20210909154333098](C:\Users\hjjan\AppData\Roaming\Typora\typora-user-images\image-20210909154333098.png)

형태로 출력 (*sample data)