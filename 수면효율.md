# 리눅스

#### 서버접속

```
ssh -p 22 root@169.56.89.210 / tkHhD4UmcQsJ
ssh -p 22 humandeep@169.56.89.210 / humandeeP@916
```



#### gpu 사용량 확인

```
nvidia-smi -l 1 (1초마다 확인)
```



#### 경로확인

```
pwd
```



#### activate

```
conda info --envs  (확인하기)
conda activate p36  (라이브러리 환경)
```



#### PGCN 후

파일돌리고 다시 결과물 출력할 시 중복 이슈를 제거 하기 위해 *.pc, *.pkl 파일, save폴더 지우기



# Video 전처리





# TSN

**Input** : Annotation, Video 전처리

dataset(ucf101, hmdb51, thumos, kinetics)에 따라 class 수 정의

TSN모델 정의	

​	_prepare_base_model (input에 따른 size, mean, std 정의)

​	prepare_tsn(마지막 layer 의 fc layer 정의)

​	consensusModule

model

​	crop_size, scale_size, input_mean, input_std 호출

​	get_optim_policies로 weight, bias 호출

​	get_augentation 실행

​    	GroupMultiScaleCrop  ->  GroupRandomHorizontalFlip 으로 crop 진행

Normalize 진행

train_loader

​	TSNDataSet 선언

​		path, list_file, num_segment, new_length, modality, image_tmpl(이미지경로), transform, random_shift, test_mode

loss function, optimizer 정의

​	CrossEntropyLoss(), SGD

train

==========training==========

partialBN = True로 지정

model의 train으로 switch

dataset.py 의 __ getitem__  실행

​	sample_indices

​		전체 영상의 frame을 segment로 나누어서 동일한 길이로 나눈다

​		이후 해당 구간에서 random한 위치에서 하나의 frame을 샘플링한다

​		구해진 segment 수만큼의 frame index를 return 한다

​	get 함수 실행

​		point를 통해서 해당 video에서 동일한 index의 image를 load 한다

​		segment 수 만큼의 이미지를 저장하고, 이를 transform 파이프라인에 넣어준다

​			transform 파이프라인

​				- GroupMultiScaleCrop	

​				- GroupRandomHorizontalFlip

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

input, target을 구하고 정의한 model 을 사용하여 training후 output, loss 구한다(model forward)

calculate loss, get accuracy

==========testing==========

해당 변수들 가져온다

tsn model 선언

cropping

​	GroupOverSample 사용

test dataloader

​	switch to test mode

dataset.py 의 __ getitem__  실행

​	test_indices

​		segment로 image sampling 수행

​	get 함수 실행

​		tranform 파이프라인

​				- GroupOversampling

​				- stack

​				- ToTorchFormatTensor

​				- GroupNormalize

test dataloader 사용해서 data와 label을 가져온다

eval_video

csv file로 저장

**Output** : *.csv





# BSN

#### TEM (Temporal Evaluation Module)

**Input** : TSN output(*.csv)

* TEM_train.py

  * X_feature, X_xmin, X_xmax, Y_bbox, Index, LR 의 placeholder 지정

  * tem_train()함수 실행

    ```
    optimizer,loss,SCNN_trainable_variables = tem_train(X_feature,X_xmin,X_xmax,Y_bbox,Index,LR,config)
    ```

    X_feature를 conv1d , sigmoid 거치고

    ```
    net = tf.layers.conv1d(inputs=X_feature,filters=512,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu)
    net = tf.layers.conv1d(inputs=net,filters=512,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu)
    net = 0.1*tf.layers.conv1d(inputs=net,filters=3,kernel_size=1,strides=1,padding='same')
    net = tf.nn.sigmoid(net)
    ```

     action, start, end로 나눈다

    Y_bbox에서 gt_xmin, gt_xmax, gt_duration 구한다

    gt_duration_boundary를 구하고

    ```
    gt_duration_boundary=tf.maximum(5.0,0.1*gt_duration)
    ```

    gt_start_bbox, gt_end_bbox구한다

    ```
    gt_start_bboxs=tf.stack((gt_xmins-gt_duration_boundary/2,gt_xmins+gt_duration_boundary/2),axis=1)
    gt_end_bboxs=tf.stack((gt_xmaxs-gt_duration_boundary/2,gt_xmaxs+gt_duration_boundary/2),axis=1)
    ```

    boundary의 1/2 전후로 늘린다

    match_scores_start, match_scores_end, match_scores_action를 구하기 위해 tem_bboxes_encode함수 실행

    ```
    match_scores_start=tem_bboxes_encode(anchors_xmin,anchors_xmax,gt_start_bboxs,Index,config)
    match_scores_end=tem_bboxes_encode(anchors_xmin,anchors_xmax,gt_end_bboxs,Index,config)
    match_scores_action=tem_bboxes_encode(anchors_xmin,anchors_xmax,Y_bbox,Index,config)
    ```

    

 

**Ouput** : [frame, action, start, end] (*.csv)



#### PGM (Proposals Generation Module)

**Input** : PEM output (*.csv)







**Output** : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



#### PEM (Proposals Evaluation Module)

**Input** : PGM output (*.csv)







**Output** : [xmin, xmax, xmin_score, xmax_score, match_iou, match_ioa, match_xmin, match, xmax] (*.csv)



* BSN_post_processing => thumos14_bsn_results.csv 생성 

​											[f-end, f-init, score, video-frames, video-name, match-iou, match-ioa, match-xmin, match-xmax]

#### Proposal list

jupyter notebook file (D:\JupyterProject\Generate_Proposal_list.ipynb)

train_proposal_list : PGM   /   test_proposal_list : PEM 으로 생성

**Output** : train_proposal_list.txt , test_proposal_list.txt



# I3D

parser

data type이 rgb인지 flow인지 확인 후 data, save폴더 경로 지정

1. rgb일때

   check point, end point, video list 불러온다

   video list에서 'flow'가 있으면 제거

   save_dir에 video list와 같은 이름의 파일이있으면 제거

   data dir에 있는 하나씩 데이터 불러와 sample(_SAMPLE)이름으로 저장

   get_RGB_feature함수 실행 

   ```
   get_RGB_feature(args.type, _SAMPLE, _CHECKPOINT_PATHS, end_point, i, save_dirs)
   ```

   data frame수에 따라 batch size 지정

   rgb_input = [1, bat_size, _IMAGE_SIZE, _IMAGE_SIZE, 3] 형태로 저장

   

   

   ​	



# PGCN

**Input** : BSN output data (proposal_list) + I3D output data (RGB, flow feature)

**PCGN_train.py**

​	model 정의

```
model = PGCN(model_configs, graph_configs)
```

​	train data 정의 (train_loader = torch.utils.data.DataLoader(PGCNDataSet()))

```
train_loader = torch.utils.data.DataLoader(
        PGCNDataSet(dataset_configs, graph_configs,
                    prop_file=dataset_configs['train_prop_file'],
                    prop_dict_path=dataset_configs['train_dict_path'],
                    ft_path=dataset_configs['train_ft_path'],
                    epoch_multiplier=dataset_configs['training_epoch_multiplier'],
                    test_mode=False),
        batch_size=args.batch_size, shuffle=True,
        num_workers=args.workers, pin_memory=True, drop_last=True)
```

​		proposal list의 proposal들을 나누고 분리해서 저장한다.

​		fore ground : incomplete : back ground = 1 : 6 : 1 로 나눈다

​		video 마다 아래 형태로 저장

​			- path

​			- frame

​			- gt_box [label, start, end]

​			- prop_box [label, iou, ioa, start, end]

​		-prepare_iou_dict() 실행 후 [act_iou_dict, act_dis_dict, prop_dict]를 pickle형태로 저장

```
pickle.dump([self.act_iou_dict, self.act_dis_dict, self.prop_dict], open(self.prop_dict_path, "wb"))
```

​			(iou_dict : 상관관계,     dis_dict : 거리)

​	loss function 선언

```
activity_criterion = torch.nn.CrossEntropyLoss().cuda()
completeness_criterion = CompletenessLoss().cuda()
regression_criterion = ClassWiseRegressionLoss().cuda()
```

​	optimizer와 weight, learning rate을 선언

```
adjust_learning_rate(optimizer, epoch, args.lr_steps)
```

train 진행

```
train(train_loader, model, activity_criterion, completeness_criterion, regression_criterion, optimizer, epoch)
```

​	**#train_loader**

​	get_training_data를 통해서 하나의 데이터를 가져온다.

​		video centric sampling 진행

​			video에서 fg, incomp, bg를 분리 -> 8번 sampling(1:6:1)

​			sampling할 type에서 랜덤으로 데이터(proposal)를 하나 추출한다

​			추출한 proposal을 루트노드라고 가정하고, 루트노드를 통해 자식노드를 sampling한다

​			추출한 루트노드와 비슷한 iou를 갖거나 dis를 가진 proposal 8개(iou), 2개(dis) sampling하고 그중 4개 랜덤 sampling 한다

​			이를 5번 반복 (4*5+1 = 21) => 총 168개 proposal사용 (fg : 21개,  incomp : 126개,  bf : 21개) 

​		load_prop_data -> **return** indices, label, reg_targets, type

​		I3D_pooling 진행

​			all_act_feature shape = 1024

​			all_comp_feature shape = 1024 * 3 [start_ft, act_ft, end_ft] = 3072

​			최종적으로 5가지 데이터를 return하여 training에 사용한다

​				- all_act_feature (1, 1024)shape

​				- all_comp_feature (1, 3072)shape

​				- proposal type -> 0, 1, 2

​				- proposal label -> 1~20 or 0

​				- 표준화된 regression target (log reg, size reg)

​	#model

​	graph convolutinal network를 통해서

​		- activity fts = batch size * 168 * 1024

​		- completeness fts = batch size * 168 * 3072

​		- target = batch size * 168

​		- reg target = batch size * 168 * 2

​		- prop type = batch size * 168

​	train_forward()

​		activity와 completeness간 각각 코사인 유사도를 측정 mask matrix생성하여 **adj marix**(5376, 5376)를 만든다

​	GCN 수행(graph convolutin 2번-mat mul)

​		(feature matrix * 정규분포 난수 weight) => (5376, 512)		(adjacency matrix * output matrix) => (5376, 512)

​		(feature matrix * 정규분포 난수 weight) => (5376, 1024)		(adjacency matrix * output matrix) => (5376, 1024)

​		동일하게 comp GCN 실행후 (5376, 3072)를 return 한다

​	Graph Convolution 의 output과 feature matrix를 concat 하고 , 21간격으로 sampling 한다

​		act_out = (256, 2048)

​		comp_out = (256, 6144)

​	각각 fully connected Layer를 통해 계산

​		activity fc = (256, 21)  => 20 class + 1 bg

​		completeness fc = (224, 20) => 20 class (bg 사용 x)

​	proposal type과 target은 기존의 (32, 168)에서 1차원으로 펴주고 21개의 간격으로 sampling

​		type = 256	target = 256

​	regression target = (5376, 2) => 21간격으로 sampling

​		(256, 2)

​	reg indexer -> type data가 0이 아닌것들만 모아서 숫자를 카운팅

​	comp fts를 regressor fc에 넣고, (256, 40)의 값을 추출 후 (256, 20, 2)형태로 reshape하여 최종 예측으로 사용

​	model을 거치고 난뒤, 전달되어 loss가 계산되는 최종 output정리

​		raw act fc -> 기존(256, 21)에서 prop type이 'bg'가 아닌것

​		act target -> target에서 bg와 fg만 계산

​		type data -> prop type[0, 1, 2]를 act 개수만큼

​		raw comp fc -> action의 무결성을 계산하여, bg를 제외한 224개의 proposal만 사용

​		comp target -> target에서 bg를 제외한 fg와 incomp만 사용, 224개의 target

​		regressor fc -> fc 이후 (256, 20, 2)의 데이터 이며, fg에 대한 회귀만 계산 256개의 데이터 중 fg인것만 사용

​		target -> 행동의 클래스 중 fg인것만 사용

​		reg target -> (256, 2)의 target data이며 표준화된 점수이다 이를 fg로 분류된 데이터만 사용

 -> 8가지 데이터를 return 해서 loss를 계산하며 PGCN모델을 training한다

 -> 모델의 전체 loss는 Act loss + (Comp loss * 0.5) + (Reg loss * 0.5)를 사용한다



**PGCN_test.py**

-> torch의 multi processing 사용해서 test를 진행, 함수를 호출해서 multiprocessing을 지정한다

1. model best check point에서 weight를 불러온다

2. model layer의 state와 regression의 평균과 분산 불러온다

3. dataloader를 통해 test data set 정의

   -> test proposal에서 video별 proposal을 읽고, 5가지 정보를 입력,저장

   -> iou dict와 dis dict를 video별로 구해서 pickle로 저장

4. 정의한 dataset과 dict를 통해서 runner func을 호출하여 multiprocessing 함수를 사용

5. PGCN model을 정의하고, trained state를 불러와서 적용

   -> dataloader를 통해 video list에서 4가지 정보를 가져온다

   1) rel proposal		2. prop ticks		3. video id		4. frame

6. 이를 통해 act와 comp의 score를 계산한다 + regression scores

7. 계산된 모델의 output을 담을 수 있게 output shape만큼 zero tensor 선언

   -> act score, comp score, reg score

8. video name, prop ticks(s_frame, e_frame), (valid_s_frame, valid_e_frame), dataset(tensor data), video frame

   I3D pooling을 통해 sampling -> act all fts[1024], comp all fts[3072]

9. 이후 model을 통해 3가지 예측치를 도출

   ->get_adjacent_batch()를 통해 인접 proposal의 정보를 sampling

10.  model 연산

    -> act와 comp에 대해 각각 유사도를 계산하고, 인접행렬을 계산하기 위해 mask array를 생성

    -> training과 동일하게 mask array에 자식, 손주 노드의 개수만큼 데이터를 input

    -> 유사도 행렬과 mask array를 곱해서 인접행렬을 구한다(relu를 통과시켜 음수 제거)

    -> GCN 연산 수행 후 graph output과 feature matrix를 concat

    -> concat 된 arrary를 통해, fc layer를 거치고 최종 output 사용

    -> **return** raw act fc [n, 21], raw comp fc [n, 20], raw regress fc [n, 20, 2]

11. 해당 regression을 다시 일반점수르 되돌린다(score * 편차 + 평균)

    -> multiprocessing을 통해 연산하고, result_queue에 하나씩 입력

12. return되는 데이터는 총 4가지 이며, 이를 dict형태로 구성, pickle로 저장 후 최종 성능평가에 사용

    {video name : 'rel_prop' : 

    ​						  'act score' :

    ​						  'comp score' :

    ​						  'reg score' : }  	

    ->mAP 계산 (evaluate detection function -> eval_detection_results.py)

threshold를 정하고 겹치는 정도를 평가



**output** : pred_dump.pc file 



# 결과 도출

jupyter notebook -> PGCN_predict.ipynb

![image-20210909154333098](C:\Users\hjjan\AppData\Roaming\Typora\typora-user-images\image-20210909154333098.png)

형태로 출력 (*sample data)